from rag.module.VectorConverter import FileToChromaIndex
from rag.module.ModelConnector import OllamaConnector

class RAGOllama:
    def __init__(self, ollama_base_url="http://localhost:11434", chroma_dir="chroma_store", model="llama3"):
        """
        Initialize the RAG pipeline with Ollama and ChromaDB.
        :param ollama_base_url: Ollama API base URL
        :param chroma_dir: Directory where ChromaDB data is stored
        :param model: Ollama model name
        """
        self.file_indexer = FileToChromaIndex(persist_dir=chroma_dir)
        self.ollama = OllamaConnector(base_url=ollama_base_url)
        self.model = model

    def build_index(self, file_path):
        """
        Build or update the index with a given file (PDF or TXT).
        """
        return self.file_indexer.create_index(file_path)

    def query_with_context(self, query_text, top_k=3):
        """
        Retrieve top-k relevant documents from the knowledge base and query Ollama with context.
        :param query_text: The user question
        :param top_k: Number of top relevant chunks to include
        :return: Answer generated by Ollama using retrieved context
        """
        if not self.file_indexer.index:
            raise RuntimeError("Index not created yet. Call build_index() first.")

        retriever = self.file_indexer.index.as_retriever(similarity_top_k=top_k)
        relevant_nodes = retriever.retrieve(query_text)

        context = "\n\n".join([node.get_content() for node in relevant_nodes])

        augmented_prompt = f"""You are a helpful assistant.
        Use the following context to answer the question:

        {context}

        Question: {query_text}
        Answer:"""

        return self.ollama.generate(model=self.model, prompt=augmented_prompt)
